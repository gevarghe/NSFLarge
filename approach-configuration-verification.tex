
\subsection{From Configurations to Policies: Extending Network Verification}

\paragraph*{Tool:} (Challenge C5, C7, Govindan, Millstein, Tamir, Varghese). A network architect must either map configurations at routers to a network policy ({\em verification}) or map a network policy to configurations at routers ({\em synthesis}) ({\bf S5}).

\paragraph*{Science:} Past work in network verification is unscalable or makes assumptions that limits its applicability; we will develop general approaches to these problems such as {\em stepwise refinement} and {\em network specific
data mining} to find bugs without specifications. We will also develop new algorithms to compactly encode the
{\em semantic difference} between two routers.

\paragraph*{Approach:}

Together with colleagues, we have helped pioneer the science of configuration verification (Header Space Analysis~\cite{hsa}); built tools for exhaustive data plane~\cite{hsa,netplumber,nod} and control plane~\cite{batfish,era} verification; and applied the idea of automated testing to data planes~\cite{atpg}.  In this thrust, we seek to improve the scalability of these tools and design missing pieces that prevent their application to a broader set of verification and synthesis problems in networking.

We propose new directions for verifying the data plane (the flow of data packets) and the control plane (the flow of routing packets).

{\em 1. Finding Bugs without Specifications:} It has now become standard practice to use formal methods and ``prove'' that the existing router configurations work correctly across all packets for both data plane (e.g., Veriflow~\cite{veriflow} and HSA~\cite{hsa}) and control plane (e.g., ERA~\cite{era}, ARC~\cite{arc}, and Minesweeper~\cite{minesweeper}).  However, this approach requires some form of specification to check the router or network model against.  In our experience, such specifications are often missing, wrong, or at best partially correct (basic beliefs
like ``Customer Machines should reach DNS'' are often correct~\cite{nod} but are only {\em partial} specifications). This is possibly because networks are managed by multiple operators, and there is also much legacy code in configurations that current operators do not understand but are scared to change.  This begs the question: can we find bugs without a specification?

The field of unsupervised learning and data mining suggests that one could {\em cluster} the set of routers in an operational network into partitions that we might term {\em roles}, confirm that roles are meaningful by looking for substantial agreement  between routers in a role, and finally spotting bugs as {\em anomalies} where some routers differ in some way from the majority of routers in the role.  We tried
straightforward approaches to clustering using off-the-shelf techniques like {\em k-means} and {\em Hamming distance} metrics, but failed to produce meaningful clusters despite tinkering with parameters.

This has led us to invent new {\em network specific clustering} algorithms. Note that the work in network verification such as Veriflow~\cite{veriflow}
and HSA~\cite{hsa} was able to scale because they exploited the structure of networks to scale verification beyond the limits and limitations of 
conventional verification tools like SAT solvers and symbolic execution applied blindly to the networking domain.  In the same way,
we believe that clustering and data mining must be rethought to exploit the structure of the network domain.

Consider, for example, the following network specific clustering that we find works quite well in practice.  We start with the
observation that most names assigned to routers in organizations are meaningful.  In UCLA, for example, a router
could be named br1.boelter.floor1, where the ``br'' stands for border router, and ``boelter'' is a building name. We have seen
similar behavior in other universities and corporations.  Thus, we divide the name into lexical tokens separated
by delimiters (such as punctutation while ignoring the numbers).  We then evaluate how meaningful each combination of tokens is as a role.  We use statistical techiques to evaluate the strength of each role as a hypothesis.  Once we find a good role partition (measured by substantial agreement in some precise sense between routers in a role), we then look for bugs as deviant configurations~\cite{engler} within each role.   

A nice feature of the above approach is its generality:  any clustering approach can be used, and statistical measures guard against poor choices.  This means that we can experiment with many different clustering algorithms and potentially employ combinations of them.  The name-based clustering technique described above is based on the reasonable assumption that human operators assign router names based on some implicit semantics.  However, not all networks employ a meaningful naming scheme, and even those that do are often not perfectly consistent.  As an alternative,
we have also investigated a role definition based on distance from the border of the AS: the nodes that interface directly with external ASes constitute one role, the nodes one hop away from those nodes constitute another role, etc.  
Our preliminary work with these approaches has identified new bugs that earlier technologies did not.  Our grant plans to flesh out these preliminary ideas, evaluate them scientifically on a much larger set of networks, and make the tools available publicly. 

{\bf 2. Synactic and Semantic Differences:} How can we tell whether a role is meaningful? For example, in UCLA it turns out that the pure router type (e.g. ``br'' for "border") is not sufficiently meaningful but the combination of type and building (e.g., ``br.boelter") is.  We measure goodness of a role by measuring the syntactic or semantic similarity of routers within the same role. A simple syntactic  check for example is to check for similar definitions, e.g., of DNS servers.  An anomaly then occurs when ``most'' (measured by a threshold) routers in a role have the same DNS server but a few do not.  

A more profound difference is to measure the semantic difference in terms of forwarding behavior of two routers across corresponding interfaces (e.g., upwards towards the ISP).  Existing network verification can verify that two routers are semantically equivalent, but if they are not equivalent only a single counterexample packet is produced~\cite{minesweeper}.  We have invented a new polynomial time algorithm that uses this existing technology as a subroutine in order to obtain the exact set of destination prefixes on which the two routers' behavior differs.  In addition to its use for partitioning nodes into roles, this algorithm could be used in any setting where it is useful to exactly characterize the differences between two nodes.  

Roughly the semantic difference algorithm is as follows.  Assume we are looking for the set of all destination prefixes that
are treated differently by two given routers.  We start by branching on prefixes that start with a 0 and those that start with a 1.  For each prefix $P$, we
ask Minesweeper~\cite{minesweeper} whether the two routers treat all destinations matching $P$ equivalently.  If Minesweeper finds they are equivalent, then we are done.  Otherwise, if there is {\em no} destination matching $P$ that the routers treat equivalently, then we return $P$.  Otherwise, we branch further, adding one more bit to the prefix $P$.

% \begin{verbatim}
% FindAllDiffs (R1, R2, P)  //* find all differences between router R1 and R2 for Destination addresses starting with Prefix P
%   If (For all addresses X that satisfies Prefix P, R1 and R2 have similar behavior) // Minesweeper call
%           return; //no differences
%   Else If (For all addresses X that satisfies Prefix P, R1 and R2 have differing behavior) // clever Z3 call invented by Alan
%            Print (P); //everything in P is a difference
%            return;
%   FindAllDiffs (R1, R2, P0); //recurse on P0
%   FindAllDiffs (R1, R2, P1); // recurse on P1
% \end{verbatim}

We have found that while the set of prefixes produced is fairly compact, one can compress further by using a bottom
up dynamic programming algorithm to also exploit the fact that many sets of prefixes can be compactly expressed
in a difference-of-cubes notation~\cite{hsa, nod}.  The algorithm walks the subtree of prefixes bottom up, evaluating the
costs of the two notations (positive form and difference form) to find the minimal representation.

{\em 3. Stateful Verification via Stepwise Refinement:} Stepwise refinement dates back to Dijkstra and Wirth~\cite{wirth}. With Ryzhyk~\cite{leonid}, we recently introduced refinement-based network programming as an intermediate stance between network verification (the user provides all configurations, the tool verifies) and synthesis (the user provides a specification, the tool synthesizes).  Instead, the user interacts with the system at each level of refinement to design and prove correctness of that level.
%
Step-wise refinement is especially promising because it may provide a way to address one of the major limitations of current tools: they cannot model stateful devices. In practice networks maintain state for several reasons (security, congestion control). However, if a router can  no longer be modeled as a function on a single packet but rather as a function on sequences of packets, the state space becomes very large~\cite{mooly-tacas16}.  We hypothesize that step-wise refinement can escape this verification conundrum analogous to way a theorem prover can outperform a model checker if the user specifies high-level invariants.   We will work with Ryzhyk at VMWare to verify Network Function Virtualization (NFV) devices such as Google's Maglev load balancer~\cite{maglev}. 

Moving to synthesis we plan to work on:

{\em 4. Synthesis for Rural Networks:}  We have already proposed extending the work in Propane on control plane synthesis from data center networks to ISPs in an NSF Medium grant (Butane~\cite{butane}) with David Walker at Princeton.  While Butane {\em extends} Propane to cover more use cases, in this grant we {\em simplify} Propane to handle simpler rural networks such as WISPs~\cite{barathwisp} (with PI Barath Raghavan). WISP operators in Africa, India, and rural US will not learn a new network policy language but may be willing to try a phone-based UI that verifies connectivity and suggests configuration changes in response to common events. This type of synthesis adds challenges not seen in the data center context, as the topological challenges are external (e.g., terrain/line of sight, spectrum, power, etc.) and the resources available to the operator are limited~\cite{zyxt18}.
%\jnf{Phone-based UI? Seriously? These operators don't even have access to a desktop or laptop machine?}

We do not propose any work in configuration synthesis in this grant because we have already proposed generalizing the work in Propane~\cite{propane} on control plane synthesis in an NSF Medium grant led by David Walker at Princeton that is joint with Millstein and Varghese. 

\subsection{From Configurations to Tests: Creating Automated Network Tests}

\paragraph*{Tool:} (Challenge C5, C7, Millstein, Tamir, Varghese). Many vendors including Microsoft and 
Google have created VM frameworks to test routers and their configurations, because verification only verifies a {\em model} at  each router.  Unfortunately, these frameworks require
the tester to manually supply tests.  Instead, we suggest automated testing of network routing.

\paragraph*{Science:}  Inspired  by our earlier work ATPG~\cite{atpg} for testing data planes and software tools for automated test generation (e.g., KLEE~\cite{klee}, we will automatically generate test {\em routing announcements} and emulate on UCLA and USC topologies.

\paragraph*{Approach:}

Unlike KLEE, ATPG gets away with a much simpler algorithm based on set cover compared to say KLEE or DART because (in bug-free networks) there are no cycles (although network verification can and should check for such cycles first before applying such methods): hence the state space is finite (though large).   KLEE and DART have a more complex set of heuristics to search the infinite 
space of executions caused by programs that can have loops.   

However, BGP and OSPF provide
an interesting intermediate ground; unlike loop-free data packet forwarding, BGP announcements can be batted back and forth before they converge
and simple set cover is unlikely to work.   On the other hand, most commonly used forms of BGP are not as complex as arbitrary code.
Thus we seek intermediate methods that are tailored to networking without the complexity and generality of
say KLEE or DART (which must handle arbitrary code).   The intellectual component of this grant largely arises from navigating this tradeoff space.  

While this approach is inspired by software testing, we are also inspired by hardware testing which has a prolific literature.  PI Tamir has worked in the field of computer architecture and will pursue the connections between hardware notions of testability and coverage, and networks.  [[Yuval add some stuff here]]
showing how 


